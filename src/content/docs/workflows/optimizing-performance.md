---
title: Optimizing Performance
description: "Documentation for Optimizing Performance
description:
section: workflows
category: workflows
order: 9
published: true"
section: workflows
category: workflows
order: 9
published: true
---

# Optimizing Performance

Learn how to identify and fix performance bottlenecks with Mekong Marketing - from profiling and analysis to implementation of caching, database optimization, and code improvements.

## Overview

**Goal**: Identify and resolve performance bottlenecks systematically
**Time**: 30-60 minutes (vs 4-12 hours manually)
**Agents Used**: debugger, code-reviewer, tester
**Commands**: /debug, /cook, /test, /fix:hard

## Prerequisites

- Application with performance issues
- Monitoring/profiling tools installed
- Performance baseline metrics
- Test data representative of production

## Performance Targets

| Metric | Good | Acceptable | Poor |
|--------|------|------------|------|
| API Response Time | <200ms | 200-500ms | >500ms |
| Page Load Time | <2s | 2-4s | >4s |
| Database Query | <50ms | 50-200ms | >200ms |
| Memory Usage | <512MB | 512MB-2GB | >2GB |
| CPU Usage | <30% | 30-70% | >70% |

## Step-by-Step Workflow

### Step 1: Identify Performance Issues

Start by profiling your application:

```bash
# Run performance analysis
/debug [analyze application performance and identify bottlenecks]
```

**Analysis output**:
```
Performance Analysis Report

ðŸ”´ Critical Issues (3)

1. Database Query Performance
   Location: src/users/service.js:23
   Issue: N+1 query problem
   Impact: 2,847ms per request
   Queries: 156 individual queries
   Recommendation: Use eager loading

2. Unoptimized Images
   Location: public/uploads/
   Issue: Large image sizes
   Impact: 3.2s additional load time
   Total: 12.4MB transferred
   Recommendation: Image optimization + CDN

3. Missing Cache Layer
   Location: src/products/service.js
   Issue: Repeated expensive operations
   Impact: 95% cache miss rate
   Recommendation: Implement Redis caching

ðŸŸ¡ Warning Issues (5)

4. Inefficient Algorithm
   Location: src/search/index.js:45
   Issue: O(nÂ²) complexity
   Impact: 523ms for 1000 items
   Recommendation: Use hash map (O(n))

5. Blocking Operations
   Location: src/emails/send.js:12
   Issue: Synchronous email sending
   Impact: 890ms blocking
   Recommendation: Use queue (Bull/Redis)

6. Large Bundle Size
   Location: frontend/dist/
   Issue: 4.2MB JavaScript bundle
   Impact: 6s parse time on mobile
   Recommendation: Code splitting

7. No Compression
   Issue: Missing gzip/brotli
   Impact: 2.8x larger transfers
   Recommendation: Enable compression

8. Memory Leaks
   Location: src/websocket/handler.js
   Issue: Event listeners not removed
   Impact: 125MB/hour growth
   Recommendation: Cleanup on disconnect

Performance Score: 34/100 (Poor)
Load Time: 8.2s (Target: <2s)
Time to Interactive: 12.4s (Target: <3.5s)
```

### Step 2: Fix Database Performance

#### N+1 Query Problem

```bash
/cook [fix N+1 query problem in user service with eager loading]
```

**Implementation**:
```
[1/4] Analyzing queries...
  âœ“ Identified 156 individual queries
  âœ“ Should be 1 query with joins

Before:
const users = await User.findAll();
for (const user of users) {
  user.posts = await Post.findAll({ userId: user.id }); // N queries
}

After:
const users = await User.findAll({
  include: [{ model: Post }] // 1 query with JOIN
});

[2/4] Implementing fix...
  âœ“ Added eager loading
  âœ“ Optimized includes
  âœ“ Added select to limit fields

[3/4] Testing...
  âœ“ Query count: 156 â†’ 1
  âœ“ Response time: 2,847ms â†’ 87ms
  âœ“ 97% improvement

[4/4] Documentation...
  âœ“ Added performance notes

âœ… N+1 problem fixed
```

#### Add Database Indexes

```bash
/cook [add database indexes for frequently queried fields]
```

**Implementation**:
```
[1/3] Analyzing query patterns...
  âœ“ Identified slow queries
  âœ“ Found missing indexes

Indexes to add:
- users.email (unique)
- posts.userId (foreign key)
- posts.status (filter)
- posts.createdAt (sorting)
- products.categoryId (foreign key)
- products.price (range queries)

[2/3] Creating indexes...
  âœ“ Generated migration
  âœ“ Composite indexes for complex queries
  âœ“ Partial indexes for filtered queries

[3/3] Testing...
  âœ“ Query performance improved:
    - User lookup: 234ms â†’ 3ms
    - Post filtering: 567ms â†’ 12ms
    - Product search: 892ms â†’ 34ms

âœ… Indexes added
```

#### Query Optimization

```bash
/cook [optimize database queries to reduce execution time]
```

### Step 3: Implement Caching

#### Redis Cache

```bash
/cook [implement Redis caching for frequently accessed data]
```

**Implementation**:
```
[1/5] Setting up Redis...
  âœ“ Installed Redis client
  âœ“ Configuration added
  âœ“ Connection pool setup

[2/5] Cache strategy...
  âœ“ Cache-aside pattern
  âœ“ Write-through for updates
  âœ“ TTL configuration

[3/5] Implementing cache...
  âœ“ User data (TTL: 1 hour)
  âœ“ Product catalog (TTL: 15 min)
  âœ“ Search results (TTL: 5 min)
  âœ“ API responses (TTL: 1 min)

[4/5] Cache invalidation...
  âœ“ On update operations
  âœ“ On delete operations
  âœ“ Manual invalidation API

[5/5] Testing...
  âœ“ Cache hit rate: 0% â†’ 87%
  âœ“ Response time: 456ms â†’ 23ms
  âœ“ Database load: -76%

âœ… Redis caching implemented

Performance improvement:
- Average response: 95% faster
- Database queries: 76% reduction
- Server load: 64% reduction
```

#### In-Memory Cache

```bash
/cook [add in-memory LRU cache for hot data]
```

#### CDN Integration

```bash
/cook [integrate CloudFlare CDN for static assets]
```

### Step 4: Optimize Frontend

#### Code Splitting

```bash
/cook [implement code splitting and lazy loading]
```

**Implementation**:
```
[1/4] Analyzing bundle...
  âœ“ Current size: 4.2MB
  âœ“ Identified heavy modules
  âœ“ Found unused dependencies

Heavy modules:
- moment.js: 287KB (use date-fns instead)
- lodash: 531KB (use individual imports)
- chart.js: 456KB (lazy load)

[2/4] Code splitting...
  âœ“ Route-based splitting
  âœ“ Component lazy loading
  âœ“ Vendor chunk optimization

[3/4] Tree shaking...
  âœ“ Removed unused code
  âœ“ Optimized imports
  âœ“ Replaced heavy libraries

[4/4] Results...
  âœ“ Bundle size: 4.2MB â†’ 687KB (84% reduction)
  âœ“ Initial load: 6s â†’ 1.2s
  âœ“ Time to interactive: 12.4s â†’ 2.8s

âœ… Frontend optimized
```

#### Image Optimization

```bash
/cook [optimize images with compression and lazy loading]
```

**Implementation**:
```
[1/4] Image analysis...
  âœ“ Total images: 342
  âœ“ Total size: 12.4MB
  âœ“ Average size: 36KB

[2/4] Optimization...
  âœ“ Convert to WebP format
  âœ“ Compress with quality 85
  âœ“ Generate responsive sizes
  âœ“ Add lazy loading

[3/4] Implementation...
  âœ“ Picture element with fallbacks
  âœ“ Intersection Observer for lazy load
  âœ“ Placeholder images

[4/4] Results...
  âœ“ Image size: 12.4MB â†’ 2.1MB (83% reduction)
  âœ“ Load time: 3.2s â†’ 0.6s
  âœ“ Bandwidth: -10.3MB per page

âœ… Images optimized
```

#### Bundle Compression

```bash
/cook [enable gzip and brotli compression]
```

### Step 5: Optimize Algorithms

#### Replace Inefficient Algorithm

```bash
/cook [replace O(nÂ²) algorithm with O(n) hash map solution]
```

**Before** (O(nÂ²) - 523ms):
```javascript
function findDuplicates(items) {
  const duplicates = [];
  for (let i = 0; i < items.length; i++) {
    for (let j = i + 1; j < items.length; j++) {
      if (items[i] === items[j]) {
        duplicates.push(items[i]);
      }
    }
  }
  return duplicates;
}
```

**After** (O(n) - 4ms):
```javascript
function findDuplicates(items) {
  const seen = new Set();
  const duplicates = new Set();

  for (const item of items) {
    if (seen.has(item)) {
      duplicates.add(item);
    } else {
      seen.add(item);
    }
  }

  return Array.from(duplicates);
}
```

**Result**: 99.2% faster (523ms â†’ 4ms)

### Step 6: Async Operations

#### Background Jobs

```bash
/cook [move email sending to background queue with Bull]
```

**Implementation**:
```
[1/4] Setting up Bull queue...
  âœ“ Redis queue configured
  âœ“ Worker processes setup
  âœ“ Job processing logic

[2/4] Moving operations to queue...
  âœ“ Email sending (was 890ms blocking)
  âœ“ Report generation (was 2.3s blocking)
  âœ“ Image processing (was 1.2s blocking)

[3/4] Implementing retry logic...
  âœ“ Automatic retry on failure
  âœ“ Exponential backoff
  âœ“ Dead letter queue

[4/4] Results...
  âœ“ API response: 890ms â†’ 45ms
  âœ“ Non-blocking operations
  âœ“ Better error handling

âœ… Background jobs implemented
```

#### Parallel Processing

```bash
/cook [process multiple operations in parallel instead of sequential]
```

### Step 7: Database Connection Pool

```bash
/cook [optimize database connection pooling]
```

**Configuration**:
```javascript
// Before: Default settings
pool: {
  max: 5,
  min: 0,
  idle: 10000
}

// After: Optimized
pool: {
  max: 20,          // More connections
  min: 5,           // Keep minimum ready
  idle: 30000,      // Longer idle time
  acquire: 60000,   // Longer acquire timeout
  evict: 1000       // Cleanup interval
}

Result: 45% faster during peak load
```

### Step 8: API Rate Limiting & Throttling

```bash
/cook [implement intelligent rate limiting and request throttling]
```

### Step 9: Memory Optimization

#### Fix Memory Leaks

```bash
/fix:hard [fix memory leak in WebSocket handler]
```

**Implementation**:
```
[1/4] Identifying leak...
  âœ“ Memory growing 125MB/hour
  âœ“ Event listeners not cleaned up
  âœ“ Socket references retained

[2/4] Implementing fixes...
  âœ“ Remove event listeners on disconnect
  âœ“ Clear socket references
  âœ“ Implement cleanup function

[3/4] Memory management...
  âœ“ WeakMap for temporary data
  âœ“ Clear timers on disconnect
  âœ“ Garbage collection hints

[4/4] Testing...
  âœ“ 24-hour test: stable memory
  âœ“ 1000 connections: no growth
  âœ“ Stress test: passed

âœ… Memory leak fixed
```

#### Reduce Memory Usage

```bash
/cook [optimize memory usage by using streams for large data]
```

### Step 10: Monitoring & Profiling

```bash
/cook [implement performance monitoring with metrics and alerts]
```

**Monitoring setup**:
```
âœ“ Response time tracking
âœ“ Database query monitoring
âœ“ Memory usage alerts
âœ“ CPU usage tracking
âœ“ Error rate monitoring
âœ“ Cache hit rate metrics
âœ“ Custom business metrics
âœ“ Real-user monitoring (RUM)

Alerts configured:
- Response time >500ms
- Error rate >1%
- Memory usage >80%
- CPU usage >75%
- Cache hit rate <70%
```

### Step 11: Load Testing

```bash
/test
```

**Performance test results**:
```
Load Test Report (1000 concurrent users)

Before optimization:
- Avg response time: 2,847ms
- 95th percentile: 5,234ms
- Requests/sec: 23
- Error rate: 12.4%
- Failed requests: 124/1000

After optimization:
- Avg response time: 87ms (97% faster)
- 95th percentile: 156ms (97% faster)
- Requests/sec: 892 (38x more)
- Error rate: 0.1%
- Failed requests: 1/1000

Database:
- Query time: 234ms â†’ 8ms (97% faster)
- Queries per request: 156 â†’ 1
- Connection pool usage: 95% â†’ 34%

Memory:
- Usage: 2.1GB â†’ 487MB (77% reduction)
- Leak rate: 125MB/hour â†’ 0MB/hour
- GC pauses: 89/hour â†’ 12/hour

Frontend:
- Bundle size: 4.2MB â†’ 687KB (84% smaller)
- Load time: 8.2s â†’ 1.2s (85% faster)
- Time to interactive: 12.4s â†’ 2.8s (77% faster)

Overall Performance Score: 34/100 â†’ 94/100

âœ… All performance targets met
```

## Complete Example: Slow E-Commerce API

### Initial Issues

```
Performance problems:
- Product listing: 4.2s response time
- Search: 6.8s with 1000 products
- Cart update: 1.8s
- Checkout: 3.4s
- Homepage: 9.2s load time
- High database load: 89% CPU
```

### Optimization Steps

```bash
# 1. Profile application
/debug [analyze e-commerce API performance]

# 2. Database optimization
/cook [fix N+1 queries and add indexes]
/cook [optimize product search queries]

# 3. Caching
/cook [implement Redis caching for products and categories]
/cook [add query result caching]

# 4. Frontend optimization
/cook [implement code splitting and lazy loading]
/cook [optimize product images with WebP and lazy loading]

# 5. API optimization
/cook [move image processing to background queue]
/cook [implement response compression]

# 6. Algorithm optimization
/cook [optimize search algorithm with inverted index]

# 7. Test improvements
/test

# 8. Monitor in production
/cook [set up performance monitoring with alerts]
```

### Results

```
After optimization (1 hour work):

Product listing: 4.2s â†’ 124ms (97% faster)
Search: 6.8s â†’ 89ms (99% faster)
Cart update: 1.8s â†’ 34ms (98% faster)
Checkout: 3.4s â†’ 187ms (95% faster)
Homepage: 9.2s â†’ 1.4s (85% faster)
Database CPU: 89% â†’ 23%

Customer impact:
- 94% faster page loads
- 10x more concurrent users
- 87% lower server costs
- 45% increase in conversions
```

### Time Comparison

**Manual optimization**: 8-16 hours
- Profiling: 1-2 hours
- Database optimization: 2-3 hours
- Caching: 2-3 hours
- Frontend: 2-4 hours
- Testing: 1-2 hours
- Debugging: 1-2 hours

**With Mekong Marketing**: 58 minutes
- Profiling: 8 minutes
- Database: 15 minutes
- Caching: 12 minutes
- Frontend: 18 minutes
- Testing: 5 minutes

**Time saved**: 7-15 hours (88% faster)

## Performance Optimization Patterns

### Pattern 1: Progressive Enhancement

```bash
/cook [implement progressive enhancement for slow connections]
```

### Pattern 2: Predictive Prefetching

```bash
/cook [add predictive prefetching for likely user actions]
```

### Pattern 3: Service Worker Caching

```bash
/cook [implement service worker for offline-first experience]
```

### Pattern 4: Database Read Replicas

```bash
/cook [set up database read replicas for scaling reads]
```

## Best Practices

### 1. Measure First

Always profile before optimizing:
```bash
âœ… Profile â†’ Identify â†’ Fix â†’ Measure
âŒ Guess â†’ Optimize â†’ Hope
```

### 2. Focus on Biggest Impact

Optimize high-impact issues first:
```
Priority order:
1. Critical path operations
2. High-frequency operations
3. User-facing operations
4. Background operations
```

### 3. Cache Aggressively

But invalidate correctly:
```javascript
// Cache layers
1. Browser cache (static assets)
2. CDN cache (global content)
3. Application cache (Redis)
4. Database query cache
5. Result memoization
```

### 4. Use Appropriate Data Structures

```javascript
âœ… Hash map for lookups: O(1)
âœ… Set for uniqueness: O(1)
âœ… Binary search: O(log n)

âŒ Array loops: O(n)
âŒ Nested loops: O(nÂ²)
```

### 5. Monitor Continuously

```bash
/cook [implement continuous performance monitoring]
```

## Troubleshooting

### Issue: Still Slow After Optimization

**Solution**:
```bash
# Re-profile
/debug [deep performance analysis with detailed metrics]

# Check for new bottlenecks
# Optimize further
```

### Issue: Cache Not Hitting

**Solution**:
```bash
/fix:fast [Redis cache hit rate below 50%]
```

### Issue: Memory Still Growing

**Solution**:
```bash
/fix:hard [memory still growing despite fixes]
```

### Issue: Database Timeout

**Solution**:
```bash
/cook [increase connection pool and optimize slow queries]
```

## Performance Checklist

```bash
Backend:
âœ“ Database queries optimized
âœ“ Indexes on frequently queried fields
âœ“ N+1 queries eliminated
âœ“ Caching implemented (Redis)
âœ“ Connection pooling optimized
âœ“ Background jobs for slow operations
âœ“ API response compression
âœ“ Rate limiting configured

Frontend:
âœ“ Code splitting implemented
âœ“ Lazy loading for routes
âœ“ Images optimized (WebP, lazy load)
âœ“ Bundle size minimized
âœ“ Tree shaking enabled
âœ“ CDN for static assets
âœ“ Service worker caching
âœ“ Critical CSS inlined

Infrastructure:
âœ“ Load balancing configured
âœ“ Auto-scaling enabled
âœ“ CDN integration
âœ“ Database read replicas
âœ“ Monitoring and alerts
âœ“ Performance budgets set
âœ“ Regular load testing

Metrics:
âœ“ Response time <200ms
âœ“ Page load <2s
âœ“ Time to interactive <3.5s
âœ“ Cache hit rate >80%
âœ“ Error rate <0.1%
```

## Next Steps

### Related Use Cases
- [Fixing Bugs](/docs/workflows/fixing-bugs) - Debug issues
- [Refactoring Code](/docs/workflows/refactoring-code) - Code quality
- [Building a REST API](/docs/workflows/building-api) - API development

### Related Commands
- [/debug](/docs/commands/core/debug) - Performance profiling
- [/cook](/docs/commands/core/cook) - Implement optimizations
- [/fix:hard](/docs/commands/fix/hard) - Complex fixes
- [/test](/docs/commands/core/test) - Performance testing

### Further Reading
- [Web.dev Performance](https://web.dev/performance/)
- [Database Indexing](https://use-the-index-luke.com/)
- [Redis Caching Patterns](https://redis.io/docs/manual/patterns/)

---

**Key Takeaway**: Mekong Marketing enables systematic performance optimization with profiling, analysis, and implementation of best practices - turning slow applications into fast ones in under an hour with measurable improvements.
